[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Project_2.html",
    "href": "Project_2.html",
    "title": "Predicting the Species",
    "section": "",
    "text": "Encelia californica and Encelia farinosa are two types of sunflower-like plants that grow in dry parts of the southwestern U.S. E. californica is more common near the coast and has greener leaves, while E. farinosa grows in the desert and has silvery, fuzzy leaves to help deal with the heat. The Fullerton Arboretum is a large garden where plants from all over the world are grown and studied. It’s a great place to learn about local plants like these two Encelia species.\nIn class, we learned about the iris dataset, which uses flower measurements to tell different iris species apart. We can do something similar with Encelia by measuring things like the number of petals or the size of the flower parts.\nKnowing how to tell these two plants apart is important for science and the environment. It helps with conservation, understanding where plants grow best, and how they might respond to climate change."
  },
  {
    "objectID": "Project_2.html#motivation-and-context",
    "href": "Project_2.html#motivation-and-context",
    "title": "Predicting the Species",
    "section": "",
    "text": "Encelia californica and Encelia farinosa are two types of sunflower-like plants that grow in dry parts of the southwestern U.S. E. californica is more common near the coast and has greener leaves, while E. farinosa grows in the desert and has silvery, fuzzy leaves to help deal with the heat. The Fullerton Arboretum is a large garden where plants from all over the world are grown and studied. It’s a great place to learn about local plants like these two Encelia species.\nIn class, we learned about the iris dataset, which uses flower measurements to tell different iris species apart. We can do something similar with Encelia by measuring things like the number of petals or the size of the flower parts.\nKnowing how to tell these two plants apart is important for science and the environment. It helps with conservation, understanding where plants grow best, and how they might respond to climate change."
  },
  {
    "objectID": "Project_2.html#main-objective",
    "href": "Project_2.html#main-objective",
    "title": "Predicting the Species",
    "section": "Main Objective",
    "text": "Main Objective\nCan we predict the species of a flower based on the number of ray florets and disk diameter using logistic regression?"
  },
  {
    "objectID": "Project_2.html#packages-used-in-this-analysis",
    "href": "Project_2.html#packages-used-in-this-analysis",
    "title": "Predicting the Species",
    "section": "Packages Used In This Analysis",
    "text": "Packages Used In This Analysis\n\nlibrary(here)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(rsample)\nlibrary(purrr)\nlibrary(yardstick)\nlibrary(tidyr)\nlibrary(caret)\n\ndf &lt;- read_csv(here::here(\"Data-1/flowers.csv\"))\n\n\n\n\nPackage\nUse\n\n\n\n\nhere\nto easily load and save data\n\n\nreadr\nto import the CSV file data\n\n\ndplyr\nto massage and summarize data\n\n\nggplot2\nto create nice-looking and informative graphs\n\n\nrsample\nto split data into training and test sets\n\n\npurrr\nto run the cross-validation\n\n\nyardstick\nto evalute the accuracy of the models\n\n\ntidyr\nto “pivot” the predictions data frame so that each row represents 1 model"
  },
  {
    "objectID": "Project_2.html#design-and-data-collection",
    "href": "Project_2.html#design-and-data-collection",
    "title": "Predicting the Species",
    "section": "Design and Data Collection",
    "text": "Design and Data Collection\nTo collect this data, our class went to the Fullerton Arboretum and paired up to collect data. Each pair was instructed to collect data on 10 flowers each and measure the following: species, number of rays, disk diameter, ray diameter, and stem lengeth. We measured everything in cm. Then we all entered our data into a spreadsheet for everyone to use.\nSome limitations that occured were the limited amount of flowers that we were able to collect data from. Also, since we were not all together, it is possible the same flower was measured twice. Another limitation could be the way everyone was measuring the stem. I noticed some people were going from the base of the flower all the way down to the root, where me and my partner only did it from the base to the end of the smaller stem that parted from the main stem."
  },
  {
    "objectID": "Project_2.html#training-test-split",
    "href": "Project_2.html#training-test-split",
    "title": "Predicting the Species",
    "section": "Training-Test Split",
    "text": "Training-Test Split\n\nlibrary(rsample)\nset.seed(123)\nsplit &lt;- initial_split(df, prop = 0.8, strata = Species)\ntrain &lt;- training(split)\ntest &lt;- testing(split)\n\nSplitting the data into training and test sets helps us see if our model can work on new data, not just the data it learned from. The training set teaches the model, and the test set checks if it can make good predictions on data it hasn’t seen before. This helps us know if the model is actually useful."
  },
  {
    "objectID": "Project_2.html#exploratory-data-analysis",
    "href": "Project_2.html#exploratory-data-analysis",
    "title": "Predicting the Species",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\n# Histogram of Ray Florets\nggplot(df, aes(x = number_rays)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\") +\n  theme_minimal() +\n  labs(title = \"Distribution of Ray Florets\", x = \"Ray Florets\")\n\n\n\n\n\n\n\n# Histogram of Disk Diameter\nggplot(df, aes(x = disk_diameter)) +\n  geom_histogram(bins = 30, fill = \"lightgreen\", color = \"black\") +\n  theme_minimal() +\n  labs(title = \"Distribution of Disk Diameter\", x = \"Disk Diameter\")\n\n\n\n\n\n\n\n\nWhat this shows: These histograms visualize how each predictor is distributed. This helps identify skewness, outliers, or transformations we might need.\nKey insights: Both features showed reasonable distributions, with slight clustering in certain ranges. This supports their potential utility in classifying flower species.\n\nggplot(df, aes(x = number_rays, y = disk_diameter, color = Species)) +\n  geom_point(size = 3, alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"Ray Florets vs Disk Diameter by Species\", \n       x = \"Ray Florets\", y = \"Disk Diameter\")\n\n\n\n\n\n\n\n\nWhat this shows: This scatterplot overlays the species onto the predictor variables. It helps us visually inspect whether species group together based on the ray florets and disk diameter.\nKey insights: Distinct clusters emerged in the plot, suggesting that flower species may be separable based on these two features.\n\ncor(df[, c(\"number_rays\", \"disk_diameter\")])\n\n              number_rays disk_diameter\nnumber_rays     1.0000000     0.5106886\ndisk_diameter   0.5106886     1.0000000\n\n\nWhat this shows: We check how closely the two predictors relate to each other. High correlation might mean redundant information, which can affect the model’s interpretation.\nKey insights:The ray florets and disk diameter showed a moderate correlation, so both were used as predictors in the model."
  },
  {
    "objectID": "Project_2.html#modeling",
    "href": "Project_2.html#modeling",
    "title": "Predicting the Species",
    "section": "Modeling",
    "text": "Modeling\n\ntrain &lt;- train |&gt; mutate(Species = factor(Species, levels = c(\"F\", \"C\")))\ntest &lt;- test |&gt; mutate(Species = factor(Species, levels = c(\"F\", \"C\")))\n\nmodel &lt;- glm(Species ~ number_rays + disk_diameter, data = train, family = \"binomial\")\n\ntest &lt;- test |&gt;\n  mutate(predicted_prob = predict(model, newdata = test, type = \"response\"),\n         predicted_class = ifelse(predicted_prob &gt; 0.5, \"C\", \"F\"))\n\nThis code trains a logistic regression model to predict flower species using the number of ray florets and disk diameter. It then uses that model to predict the species of flowers in the test set, assigning each one to species C if the predicted probability is greater than 0.5, or F otherwise. This helps us see how well the model performs on new, unseen data.\n\nlibrary(ggplot2)\n\nggplot(test, aes(x = number_rays, y = disk_diameter, color = predicted_class)) +\n  geom_point() +\n  labs(title = \"Predicted Species Based on Flower Measurements\")\n\n\n\n\n\n\n\n\nThis code creates a scatter plot to show how the model classified each flower in the test set based on its number of ray florets and disk diameter. Each point is colored by the predicted species, helping us visually assess how well the model separates the two species using these two features.\nWhat is Logistic Regression?\nLogistic regression is a type of statistical model used to predict categorical outcomes—in our case, flower species (either “F” or “C”). It estimates the probability that a flower belongs to a specific species based on its measurements. We’re using logistic regression because our target variable is binary, and it’s interpretable, fast, and works well for simple classification problems like this one.\nWhy did we chose each model that we are considering?\nWe chose a logistic regression model because we are trying to classify each flower into one of two species, the relationship between the features (number_rays, disk_diameter) and the outcome seems to be linear and well-separated in our exploratory plots, and logistic regression is an ideal starting point when the number of features is small and you want a simple, explainable model.\nWhy are you using cross-validation? How does it work?\nCross-validation helps us check how well our model performs on unseen data. It works by first, splitting the training data into smaller subsets. Then, training the model on some folds and testing it on the others, then repeating this process multiple times and averaging the results.\nThis helps prevent overfitting and gives us a more reliable estimate of how the model will perform on real-world data.\nWhich model are you selecting as the best model? Why?\nBased on simplicity, interpretability, and performance on the training data (and visual separation in EDA), we selected logistic regression as the best model. It performed well in distinguishing the species using only two features, and its predictions matched the natural groupings we observed in the data.\n\ntrain &lt;- train |&gt; mutate(Species = factor(Species, levels = c(\"F\", \"C\")))\ntest &lt;- test |&gt; mutate(Species = factor(Species, levels = c(\"F\", \"C\")))\n\ncontrol &lt;- trainControl(method = \"cv\", number = 10, classProbs = TRUE)\n\ncv_model &lt;- train(\n  Species ~ number_rays + disk_diameter,\n  data = train,\n  method = \"glm\",\n  family = \"binomial\",\n  trControl = control\n)\n\nprint(cv_model)\n\nGeneralized Linear Model \n\n79 samples\n 2 predictor\n 2 classes: 'F', 'C' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 70, 71, 72, 71, 71, 71, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.7849206  0.5633636\n\ntest$predicted_prob &lt;- predict(cv_model, newdata = test, type = \"prob\")[, \"C\"]\ntest$predicted_class &lt;- ifelse(test$predicted_prob &gt; 0.5, \"C\", \"F\")\n\naccuracy &lt;- mean(test$predicted_class == test$Species)\nprint(paste(\"Test Set Accuracy:\", round(accuracy * 100, 2), \"%\"))\n\n[1] \"Test Set Accuracy: 71.43 %\"\n\n\n\nggplot(test, aes(x = number_rays, y = disk_diameter, color = predicted_class)) +\n  geom_point() +\n  labs(title = \"Predicted Species Based on Flower Measurements\",\n       x = \"Number of Ray Florets\", y = \"Disk Diameter\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe used 10-fold cross-validation to train our logistic regression model. This method splits the training data into 10 parts, trains on 9 parts, and tests on the remaining one—repeating this process to get a reliable performance estimate. Our final model achieved high accuracy on the test set, confirming that it’s a good fit for predicting flower species based on ray florets and disk diameter."
  },
  {
    "objectID": "Project_2.html#insights",
    "href": "Project_2.html#insights",
    "title": "Predicting the Species",
    "section": "Insights",
    "text": "Insights\n\ntest &lt;- test |&gt;\n  mutate(correct = ifelse(predicted_class == Species, \"Correct\", \"Incorrect\"))\n\nggplot(test, aes(x = number_rays, y = disk_diameter, color = correct, shape = Species)) +\n  geom_point(size = 3, alpha = 0.8) +\n  labs(title = \"Correct vs Incorrect Predictions\",\n       x = \"Number of Ray Florets\", y = \"Disk Diameter\") +\n  scale_color_manual(values = c(\"Correct\" = \"darkgreen\", \"Incorrect\" = \"red\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhat this shows: Each point represents a flower. Green = correct and Red = incorrect. The plot helps identify where mistakes are happening in the feature space.\nWhy were some misclassified?\nThere are a few possibilities:\n\nOverlap in features: Some flowers from different species may have similar ray floret counts and disk diameters, causing the model to get confused.\nOutliers: A few data points might lie in unexpected places due to measurement noise or natural variation."
  },
  {
    "objectID": "Project_1.html",
    "href": "Project_1.html",
    "title": "Project 1",
    "section": "",
    "text": "Hospitals get rated based on how well they take care of their patients, and these ratings can impact their reputation and even how much funding they receive. People also look at these ratings when deciding which hospital to go to. By analyzing this data, we can figure out why some hospitals do better than others.\nIn this project, I’m using hospital survey data (HCAHPS) to explore how hospitals are rated, what factors might affect those ratings, and how ratings differ from state to state. The main goal is to understand what makes hospitals perform well and maybe even predict their ratings based on what patients say."
  },
  {
    "objectID": "Project_1.html#motivation-and-context",
    "href": "Project_1.html#motivation-and-context",
    "title": "Project 1",
    "section": "",
    "text": "Hospitals get rated based on how well they take care of their patients, and these ratings can impact their reputation and even how much funding they receive. People also look at these ratings when deciding which hospital to go to. By analyzing this data, we can figure out why some hospitals do better than others.\nIn this project, I’m using hospital survey data (HCAHPS) to explore how hospitals are rated, what factors might affect those ratings, and how ratings differ from state to state. The main goal is to understand what makes hospitals perform well and maybe even predict their ratings based on what patients say."
  },
  {
    "objectID": "Project_1.html#main-objective",
    "href": "Project_1.html#main-objective",
    "title": "Project 1",
    "section": "Main Objective",
    "text": "Main Objective\nThis project looks at how satisfied patients are with hospitals in the U.S. using data from the HCAHPS survey. It compares hospital star ratings across different places and checks how the number of survey responses and response rates might affect those ratings. In the end, I build a model to try and predict hospital star ratings based on key parts of the survey."
  },
  {
    "objectID": "Project_1.html#packages-used-in-this-analysis",
    "href": "Project_1.html#packages-used-in-this-analysis",
    "title": "Project 1",
    "section": "Packages Used In This Analysis",
    "text": "Packages Used In This Analysis\n\nhere::i_am(\"Project_1.qmd\")\n\nhere() starts at /Users/krystalmaxey/Documents/Math 237/projects\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rsample)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(broom)\nlibrary(yardstick)\n\n\nAttaching package: 'yardstick'\n\nThe following object is masked from 'package:readr':\n\n    spec\n\nlibrary(probably)\n\n\nAttaching package: 'probably'\n\nThe following objects are masked from 'package:base':\n\n    as.factor, as.ordered\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ dials        1.4.0     ✔ recipes      1.3.0\n✔ infer        1.0.7     ✔ tune         1.3.0\n✔ modeldata    1.4.0     ✔ workflows    1.2.0\n✔ parsnip      1.3.1     ✔ workflowsets 1.1.0\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nhospital &lt;- read_csv(here::here(\"Data-1/HCAHPS-Hospital.csv\"), na = c(\"Not Applicable\", \"Not Available\"))\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 443517 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (16): Facility ID, Facility Name, Address, City/Town, State, ZIP Code, C...\ndbl  (6): Patient Survey Star Rating, Patient Survey Star Rating Footnote, H...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nPackages Used\nlibrary(tidyverse): A collection of R packages for data manipulation, visualization, and functional programming using a consistent grammar and syntax.\nlibrary(rsample): Provides tools for creating resampling objects (like training/testing splits) to support modeling workflows.\nlibrary(janitor): Simplifies data cleaning tasks such as renaming columns, removing empty rows/columns, and formatting messy datasets.\nlibrary(broom): Converts statistical analysis objects into tidy tibbles for easy manipulation and visualization.\nlibrary(yardstick): Provides functions to calculate and compare model performance metrics like accuracy, precision, and recall.\nlibrary(probably): Enhances classification modeling by helping with post-processing of class probabilities, like threshold tuning.\nlibrary(tidymodels): A framework for building, tuning, and evaluating machine learning models using a unified and tidy syntax."
  },
  {
    "objectID": "Project_1.html#design-and-data-collection",
    "href": "Project_1.html#design-and-data-collection",
    "title": "Project 1",
    "section": "Design and Data Collection",
    "text": "Design and Data Collection\nThis code is cleaning up the hospital data to make it easier to work with. First, it fixes the column names so they’re all in a simple format. Then it makes a new version of the data by removing extra info like the hospital’s address and phone number that isn’t needed for the analysis.\n\nhospital &lt;- hospital |&gt;\n  clean_names()\n\nhospital_1 &lt;- hospital |&gt;\n  select(\n    !c(\"address\", \"city_town\",\"telephone_number\", \"hcahps_answer_description\") # Not Finished\n  )\n\nThis code splits the hospital data into three separate datasets so it’s easier to analyze different parts. One dataset keeps only the star ratings, another keeps the answer percentages, and the third keeps the linear mean values. Each one also removes rows where the main value (like the star rating or answer percent) is missing.\n\nhospital_star_rating &lt;- hospital_1 |&gt;\n  select(\n    !c(\"hcahps_answer_percent\", \"hcahps_answer_percent_footnote\", \"hcahps_linear_mean_value\" )\n  ) |&gt;\n  filter(\n    !is.na(`patient_survey_star_rating`)\n    )\n\n\nhospital_ans_perc &lt;- hospital_1 |&gt;\n  select(\n    !c(\"patient_survey_star_rating\", \"patient_survey_star_rating_footnote\", \"hcahps_linear_mean_value\" )\n  ) |&gt;\n  filter(\n    !is.na(`hcahps_answer_percent`)\n    )\n\n\nhospital_linear_mean &lt;- hospital_1 |&gt;\n  select(\n    !c(\"hcahps_answer_percent\", \"hcahps_answer_percent_footnote\", \"patient_survey_star_rating\", \"patient_survey_star_rating_footnote\")\n  ) |&gt;\n  filter(\n    !is.na(`hcahps_linear_mean_value`)\n    )\n\nNext, we changed the shape of the data so that each HCAHPS measure has its own column instead of being stacked in rows. This makes it easier to compare different survey results for each hospital using their facility ID.\n\nhospital_star_rating_1 &lt;- hospital_star_rating |&gt;\n  pivot_wider(id_cols = `facility_id`,\n              names_from = `hcahps_measure_id`,\n              values_from = `patient_survey_star_rating`)\n\nhospital_ans_perc_1 &lt;- hospital_ans_perc |&gt;\n  pivot_wider(id_cols = `facility_id`,\n              names_from = `hcahps_measure_id`,\n              values_from = `hcahps_answer_percent`)\n\nhospital_linear_mean_1 &lt;- hospital_linear_mean |&gt;\n  pivot_wider(id_cols = `facility_id`,\n              names_from = `hcahps_measure_id`,\n              values_from = `hcahps_linear_mean_value`)\n\nFinally, this step puts all the reshaped datasets together into one big table using the facility ID. It also removes any rows with missing info so the data is clean and ready to analyze.\n\nhospital_full &lt;- hospital_star_rating_1 |&gt;\n  inner_join(hospital_ans_perc_1, by = \"facility_id\") |&gt;\n  inner_join(hospital_linear_mean_1, by = \"facility_id\")\n\nhospital_full_clean &lt;- hospital_full |&gt;\n  drop_na()"
  },
  {
    "objectID": "Project_1.html#exploratory-data-analysis",
    "href": "Project_1.html#exploratory-data-analysis",
    "title": "Project 1",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\nhospital_star_rating |&gt;\n  filter(!is.na(patient_survey_star_rating)) |&gt;\n  ggplot(aes(x = patient_survey_star_rating)) +\n  geom_histogram(binwidth = 1, fill = \"steelblue\", color = \"white\") +\n  labs(title = \"Distribution of Patient Survey Star Ratings\", x = \"Star Rating\", y = \"Count\")\n\n\n\n\n\n\n\n\nFigure 1: This histogram shows how hospital star ratings from patient surveys are spread out. The way the ratings are grouped can give us insight into how patients generally feel or if there’s a pattern in how ratings are given.\nFinding: Most hospitals have around a 3-star rating, which suggests average patient satisfaction. Only a few hospitals get very low (1-star) or very high (5-star) ratings, meaning there isn’t a lot of extreme feedback either way.\n\nggplot(hospital_full_clean, aes(x = H_STAR_RATING)) +\n  geom_bar(fill = \"steelblue\") +\n  labs(title = \"Distribution of Hospital Star Ratings\",\n       x = \"Star Rating\", y = \"Count\")\n\n\n\n\n\n\n\n\nFigure 2: This plot shows how hospital star ratings are spread across different facilities. It helps us see if the ratings tend to be higher, lower, or spread out evenly.\nFinding: Just like the patient survey ratings, overall hospital ratings mostly fall in the middle range. This suggests that most hospitals provide similar levels of care, with fewer hospitals standing out as either exceptionally good or poor.\n\nhospital |&gt;\n  filter(!is.na(patient_survey_star_rating)) |&gt;\n  group_by(state) |&gt;\n  summarise(avg_rating = mean(patient_survey_star_rating, na.rm = TRUE)) |&gt;\n  slice_max(avg_rating, n = 5) |&gt;\n  ggplot(aes(x = reorder(state, avg_rating), y = avg_rating)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Top 5 States by Average Patient Survey Star Rating\",\n       x = \"State\", y = \"Average Star Rating\")\n\n\n\n\n\n\n\n\nFigure 3: This chart ranks states based on their average patient survey star ratings, helping us spot regional differences in patient satisfaction.\nFinding: There’s a noticeable variation in average ratings from state to state. Some states consistently have higher ratings, which could point to differences in hospital care quality or varying patient expectations across regions.\n\n# Get top 5 states by average patient survey star rating\ntop_states &lt;- hospital |&gt;\n  filter(!is.na(patient_survey_star_rating)) |&gt;\n  group_by(state) |&gt;\n  summarise(avg_rating = mean(patient_survey_star_rating, na.rm = TRUE)) |&gt;\n  slice_max(avg_rating, n = 5) |&gt;\n  pull(state)  # extract vector of state names\n\n# Plot only those states\nhospital_full_clean |&gt;\n  left_join(hospital, by = \"facility_id\") |&gt;\n  filter(state %in% top_states) |&gt;\n  ggplot(aes(x = state, y = H_STAR_RATING)) +\n  geom_boxplot() +\n  coord_flip() +\n  labs(title = \"Star Ratings for Top 5 States by Patient Survey Rating\",\n       y = \"Star Rating\", x = \"State\")\n\n\n\n\n\n\n\n\nFigure 4: This boxplot shows how star ratings vary across states, highlighting those with consistent ratings and others with more extreme differences.\nFinding: Some states have ratings that are closely grouped together, meaning hospitals there tend to perform similarly. Other states have more spread-out ratings, suggesting there are bigger differences in care quality within those states.\n\nhospital |&gt;\n  filter(!is.na(patient_survey_star_rating), !is.na(number_of_completed_surveys)) |&gt;\n  ggplot(aes(x = number_of_completed_surveys, y = patient_survey_star_rating)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Star Rating vs Number of Completed Surveys\",\n       x = \"Completed Surveys\", y = \"Star Rating\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFigure 5: This visualization looks at whether hospitals with more survey responses have different ratings, which could show if the number of responses affects the reliability of the ratings.\nFinding: There’s no clear connection between the number of completed surveys and the star rating. This indicates that the rating is not significantly influenced by how many people respond to the survey.\n\nhospital_full_clean |&gt;\n  left_join(hospital |&gt; select(facility_id, survey_response_rate_percent), \n            by = \"facility_id\") |&gt;\n  ggplot(aes(x = survey_response_rate_percent, y = H_STAR_RATING)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Survey Response Rate vs Star Rating\",\n       x = \"Survey Response Rate (%)\", y = \"Star Rating\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFigure 6: This plot examines whether higher response rates are linked to better ratings, which could indicate more engaged patients or hospitals with better practices.\nFinding: There’s a slight positive trend, suggesting that hospitals with higher response rates tend to have slightly better ratings. This could be due to better patient engagement or more transparent hospital practices."
  },
  {
    "objectID": "Project_1.html#modeling",
    "href": "Project_1.html#modeling",
    "title": "Project 1",
    "section": "Modeling",
    "text": "Modeling\nIn this step, we’re preparing the data and creating a model to predict hospital star ratings.\n\nSelecting data: We choose the important columns related to hospital performance (like cleanliness and recommendation scores) and remove any rows with missing information.\nSplitting the data: We split the data into two parts: 80% for training the model and 20% for testing how well the model works.\nBuilding the model: We create a linear regression model to predict hospital star ratings based on the other scores. We train the model using the training data.\nThis helps us predict hospital star ratings using the selected data.\n\n\nmodel_data &lt;- hospital_full_clean |&gt;\n  select(H_STAR_RATING,\n         H_HSP_RATING_LINEAR_SCORE,\n         H_RECMND_LINEAR_SCORE,\n         H_CLEAN_LINEAR_SCORE,\n         H_COMP_1_LINEAR_SCORE,\n         H_COMP_2_LINEAR_SCORE,\n         H_COMP_3_LINEAR_SCORE) |&gt;\n  drop_na()\n\n\nset.seed(123)\ndata_split &lt;- initial_split(model_data, prop = 0.8)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\n\nlm_model &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(H_STAR_RATING ~ ., data = train_data)\n\n\ntidy(lm_model) |&gt;\n  arrange(desc(abs(estimate)))\n\n# A tibble: 7 × 5\n  term                      estimate std.error statistic  p.value\n  &lt;chr&gt;                        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)               -20.7      0.346      -59.8  0       \n2 H_COMP_1_LINEAR_SCORE       0.0817   0.00694     11.8  3.37e-31\n3 H_COMP_2_LINEAR_SCORE       0.0751   0.00480     15.6  9.96e-53\n4 H_COMP_3_LINEAR_SCORE       0.0398   0.00299     13.3  4.25e-39\n5 H_HSP_RATING_LINEAR_SCORE   0.0319   0.00814      3.92 8.96e- 5\n6 H_RECMND_LINEAR_SCORE       0.0212   0.00503      4.23 2.45e- 5\n7 H_CLEAN_LINEAR_SCORE        0.0207   0.00208      9.93 8.07e-23\n\n\n\nHow to Interpret\n\nestimate: This is the effect size. For example, an estimate of 0.02 for H_RECMND_LINEAR_SCORE means that for each 1 point increase in that score, the predicted star rating increases by 0.02.\np-value: If it is below 0.05, that predictor is statistically significant.\nHighest absolute estimate values = strongest impact on the prediction.\n\n\ncoef_plot_data &lt;- tidy(lm_model) |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  mutate(term = str_replace(term, \"_LINEAR_SCORE\", \"\"), \n         term = str_replace(term, \"H_\", \"\"),              \n         term = str_replace(term, \"COMP_\", \"COMP \"),       \n         term = fct_reorder(term, abs(estimate)))\n\nggplot(coef_plot_data, aes(x = term, y = estimate, fill = estimate &gt; 0)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  labs(title = \"Predictor Effects on Hospital Star Rating\",\n       x = \"Predictor\",\n       y = \"Coefficient Estimate\") +\n  scale_fill_manual(values = c(\"steelblue\", \"steelblue\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 7: Effects of Patient Survey Measures on Hospital Star Ratings. This shows the coefficients from a linear regression model predicting hospital star ratings based on patient survey scores. The bars represent how much the star rating is expected to change with a one-unit increase in each survey measure. All variables shown are statistically significant (p &lt; 0.001).\nFinding: The analysis reveals that certain patient survey measures have a strong impact on hospital star ratings. Specifically, improvements in survey measures like patient communication, cleanliness, and responsiveness are associated with higher star ratings. The significant relationship between these factors indicates that hospitals focusing on these areas may see improved ratings.\nThe purpose of this next chunk of code is to build and evaluate a linear regression model that predicts a hospital’s star rating (H_STAR_RATING) based on several performance scores, such as cleanliness and patient recommendation scores. The code selects relevant data, splits it into training and testing sets, trains the model using the training data, makes predictions on the test data, and then measures how accurate the predictions are. Finally, it creates a graph to compare the predicted ratings with the actual ratings.\n\nmodel_data &lt;- hospital_full_clean |&gt;\n  select(H_STAR_RATING,\n         H_HSP_RATING_LINEAR_SCORE,\n         H_RECMND_LINEAR_SCORE,\n         H_CLEAN_LINEAR_SCORE,\n         H_COMP_1_LINEAR_SCORE,\n         H_COMP_2_LINEAR_SCORE,\n         H_COMP_3_LINEAR_SCORE) |&gt;\n  drop_na()\n\n\nset.seed(123)\ndata_split &lt;- initial_split(model_data, prop = 0.8)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\n\nlm_model &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(H_STAR_RATING ~ ., data = train_data)\n\n\npredict_test &lt;- predict(lm_model, new_data = test_data) |&gt;\n  bind_cols(test_data)\n\n\nmetrics(predict_test, truth = H_STAR_RATING, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.340\n2 rsq     standard       0.864\n3 mae     standard       0.277\n\nggplot(predict_test, aes(x = H_STAR_RATING, y = .pred)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") +\n  labs(title = \"Predicted vs Actual Hospital Star Ratings\",\n       x = \"Actual Rating\", y = \"Predicted Rating\")\n\n\n\n\n\n\n\n\nFigure 8: This comparison evaluates how well the model predicts star ratings using HCAHPS linear scores. Points close to the diagonal line indicate better predictions.\nFinding: The model predicts star ratings with moderate accuracy. While the predicted ratings are generally close to the actual ratings, there are some deviations, suggesting that while linear scores are helpful, they don’t account for all factors that influence hospital ratings."
  },
  {
    "objectID": "Project_1.html#conclusion",
    "href": "Project_1.html#conclusion",
    "title": "Project 1",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis looks at hospital performance using HCAHPS survey data and reveals some interesting trends. Most hospitals have average patient satisfaction ratings, with many receiving around 3 stars. This suggests that while hospitals generally provide decent care, very few stand out with either very high or very low ratings. Hospital star ratings follow a similar pattern, indicating that the level of care is relatively consistent across different hospitals.\nThe analysis also highlights that hospitals in certain states perform better than others, showing that the quality of care can differ by region. These findings are important because they emphasize the need to understand both patient satisfaction and the factors influencing hospital ratings, such as response rates and the number of surveys completed. Moving forward, it could be valuable to develop models that predict a hospital’s rating based on survey responses, offering a clearer picture of how hospitals are performing."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "projects",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  }
]